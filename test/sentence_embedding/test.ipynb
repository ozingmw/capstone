{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310    프리랜서로 일하면서 요즘 일이 없는데 가족들은 내가 열심히 하지 않은 탓이라고 해서...\n",
       " 311     오늘 은퇴하고 왔는데 가족들이 그동안 고생했다며 나를 격려해주지 않아서 마음이 아팠어.\n",
       " 312               앞으로 노후를 위해 준비할 것들이 많은데 의지할 사람은 없고 막막해.\n",
       " 313         얼마 전 은행 계좌를 봤는데 생각보다 모아 둔 돈이 없더라고. 너무 충격이었어.\n",
       " 314     그동안 담배를 너무 많이 피웠나 봐. 이제 와서 내가 모은 돈을 보니 너무 후회가 돼.\n",
       " Name: text, dtype: object,\n",
       " 310    상처\n",
       " 311    상처\n",
       " 312    상처\n",
       " 313    상처\n",
       " 314    슬픔\n",
       " Name: label, dtype: object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "train_data = pd.read_csv(\"../../datasets/train.csv\", index_col = 0, encoding = 'utf-8')\n",
    "\n",
    "train_data.dropna(inplace = True)\n",
    "train_data.drop(['감정_소분류','시스템문장1','시스템문장2','시스템문장3', '사람문장2', '사람문장3', '신체질환'], axis=1, inplace=True)\n",
    "train_data.rename({'사람문장1':'text', '감정_대분류':'label'}, axis=1, inplace=True)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# train_data['label'] = label_encoder.fit_transform(train_data['label'])\n",
    "\n",
    "X_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_train[5:10], y_train[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.8967e-03, -5.8443e-01,  8.2720e-02, -1.0768e+00,  7.3777e-01,\n",
      "         -5.1994e-01, -5.0743e-03, -1.4163e-01,  2.6563e-01, -1.0201e-01,\n",
      "         -1.9327e-01, -5.3238e-01,  2.4999e-02, -5.3367e-01,  2.9292e-01,\n",
      "         -2.5706e-01,  1.4655e-02,  1.1043e-01,  1.8759e-01,  8.9623e-02,\n",
      "         -1.7414e-01,  1.8840e-01, -1.8659e-01,  5.6257e-01, -1.2248e-01,\n",
      "          6.4579e-01,  1.8320e-01, -4.8350e-01,  3.9719e-01, -5.8897e-01,\n",
      "         -2.3307e-01,  5.6784e-01, -9.3604e-01, -1.9653e-01,  1.2081e+00,\n",
      "         -2.1208e-01, -2.6233e-01, -1.0585e-01, -3.6606e-01, -1.3617e-01,\n",
      "          5.6181e-01,  2.6937e-01, -1.6141e-01,  5.2013e-01, -6.5507e-01,\n",
      "         -1.3590e-01,  3.6612e-01,  2.2168e-01, -1.5250e-01, -9.8941e-02,\n",
      "          1.7357e-01, -6.0483e-01, -4.3323e-02,  8.5040e-01,  4.3428e-01,\n",
      "         -2.8558e-01,  5.0399e-02, -3.9722e-01,  9.6215e-01, -2.8339e-01,\n",
      "          3.7969e-01, -4.0831e-01, -8.9945e-01, -2.0159e-01,  1.6754e-01,\n",
      "         -2.1764e-01, -1.2400e-01, -4.6040e-01, -9.8380e-01, -2.9100e-01,\n",
      "         -2.1686e-01,  9.8585e-01,  6.6311e-02, -1.5005e-01,  6.3663e-01,\n",
      "         -3.9713e-01, -4.4133e-01,  2.6734e-01, -1.6287e-01,  6.9950e-02,\n",
      "         -4.1572e-02,  5.0935e-02, -4.7502e-03, -7.6903e-01,  1.8094e-01,\n",
      "         -4.8466e-01,  2.6625e-01, -2.8845e-01, -2.9571e-01, -4.5038e-01,\n",
      "         -5.3354e-02,  8.4675e-02,  3.6365e-02, -3.8294e-02,  6.9351e-01,\n",
      "          3.0328e-01,  2.7941e-01,  2.8198e-01,  6.7416e-01,  5.3589e-02,\n",
      "         -3.1146e-02, -3.1358e-01, -3.2934e-01,  3.0209e-02,  5.0882e-01,\n",
      "         -3.2734e-01, -5.4653e-02,  6.3871e-01, -8.2618e-01, -4.6821e-01,\n",
      "         -6.2531e-01,  5.8386e-01,  4.6478e-01, -2.6816e-01, -1.3566e-01,\n",
      "         -6.0145e-02, -3.3425e-01,  6.8037e-01,  1.0342e+00, -5.8053e-01,\n",
      "          3.5686e-01, -1.6222e+00,  1.5989e-01,  9.5312e-01, -8.6256e-01,\n",
      "         -2.5316e-01,  2.3472e-01,  7.1033e-03,  2.4972e-01, -6.0017e-01,\n",
      "         -5.1301e-01, -4.3912e-01,  1.3522e-01, -1.4287e-01, -1.4983e-01,\n",
      "         -7.8103e-01, -6.9815e-01,  3.2251e-01,  1.3436e-01, -5.7756e-02,\n",
      "          9.8061e-02, -6.0937e-01,  4.3404e-01, -8.6508e-01,  1.0472e+00,\n",
      "          5.7507e-01,  2.5216e-01,  2.6980e-01, -1.1408e-01, -3.1366e-01,\n",
      "          6.3194e-01, -3.4699e-02,  1.6136e-01, -2.5799e-01,  5.6641e-01,\n",
      "         -9.8088e-01, -3.7708e-01, -4.9296e-01,  3.7658e-01,  2.7880e-01,\n",
      "         -1.2706e-01, -2.8033e-01, -2.2942e-01,  2.5692e-01, -2.0532e-01,\n",
      "         -9.6173e-01, -4.5034e-01, -8.2248e-01,  7.7774e-02,  1.7720e-01,\n",
      "          3.1610e-01, -2.8160e-01,  4.2373e-01,  4.9201e-01, -2.0661e-01,\n",
      "          4.2068e-02, -1.8946e-01,  2.9951e-02,  6.1678e-01, -2.6381e-01,\n",
      "          3.9057e-01, -1.8325e-01,  1.6216e-01,  6.0595e-01, -6.5946e-01,\n",
      "          6.7217e-01, -6.6210e-01, -2.4116e-02,  3.5402e-01,  4.5734e-02,\n",
      "         -1.1800e-01,  8.5572e-01, -1.9670e-01,  4.0653e-01,  6.0815e-01,\n",
      "         -8.5092e-02, -2.9842e-01,  2.5115e-01, -2.8096e-01,  1.4468e-01,\n",
      "         -5.9906e-01,  5.2977e-01, -3.8195e-02, -2.3544e-01,  3.8774e-01,\n",
      "          2.7769e-01,  3.7260e-01,  1.1577e-01,  5.9274e-01,  2.3491e-01,\n",
      "          1.0384e-01,  4.9224e-01,  2.8700e-01,  1.4450e-01,  3.4023e-01,\n",
      "         -4.1480e-01, -4.0956e-01,  1.8105e-01, -5.9073e-01,  1.8846e-01,\n",
      "         -4.2823e-01, -6.3120e-01,  3.7744e-01, -9.0859e-02,  5.9680e-01,\n",
      "         -3.9993e-01,  2.9310e-01, -7.1620e-01, -3.5792e-01, -2.4947e-01,\n",
      "         -4.8236e-01,  6.6858e-01,  7.7703e-02, -8.3517e-01, -2.9733e-01,\n",
      "         -7.2844e-01, -1.6649e-01, -1.5071e-01,  4.8802e-01,  1.7713e-01,\n",
      "          1.2230e+00, -6.2654e-02,  5.9202e-01,  6.7526e-01, -3.1000e-01,\n",
      "         -5.1123e-01,  4.4475e-02, -2.0987e-01,  3.3290e-01, -2.3397e-01,\n",
      "         -2.2071e-01,  7.0185e-02, -2.2930e-01,  6.2451e-01, -7.1291e-01,\n",
      "          4.8863e-02, -3.1851e-01,  3.2093e-02, -5.7529e-02,  2.4105e-01,\n",
      "          6.1338e-03,  2.3208e-01, -3.7486e-01, -1.5354e-04, -1.2802e-01,\n",
      "         -7.1639e-01,  2.7455e-01, -3.9135e-01,  6.9555e-01,  4.4677e-03,\n",
      "          4.2015e-01,  1.1979e-01,  2.8923e-02,  2.1144e-01,  1.1792e-01,\n",
      "         -2.9478e-01,  7.1756e-01,  2.5356e-01,  6.7433e-01, -6.2704e-02,\n",
      "          8.8295e-01,  5.1893e-02, -1.2907e-01, -5.0617e-01, -4.8741e-01,\n",
      "         -5.6863e-01,  1.8452e-01,  4.1279e-01,  1.1240e-01, -8.4995e-01,\n",
      "          1.0577e-01,  9.2657e-01, -2.6398e-01, -3.7953e+00,  1.9668e-01,\n",
      "         -3.7461e-01,  3.2650e-02,  6.7680e-01, -5.1744e-01, -5.3826e-01,\n",
      "          2.0631e-01, -1.5054e-01,  5.8072e-01,  3.2826e-01,  2.3440e-01,\n",
      "          1.3628e-02, -1.9809e-02,  3.8517e-02, -4.4293e-01,  2.4315e-01,\n",
      "          4.6555e-01, -4.2745e-01,  9.5486e-01, -7.6359e-01,  1.3285e-01,\n",
      "         -3.8465e-01, -2.2268e-01,  2.6303e-01, -2.9251e-01, -9.0803e-01,\n",
      "          7.2331e-02,  2.6234e-01,  7.9063e-02,  4.2859e-01,  4.6184e-01,\n",
      "         -1.0033e-01, -5.9040e-02, -4.9520e-01, -9.6116e-01,  5.7474e-01,\n",
      "         -7.3278e-01, -3.9277e-01,  4.8864e-01, -2.9645e-01,  8.3188e-02,\n",
      "          6.7497e-02,  5.8156e-01, -2.1614e-01, -4.4619e-01,  9.5018e-01,\n",
      "         -8.2938e-02,  7.6955e-01,  3.7747e-01, -5.2632e-01,  6.4077e-01,\n",
      "          2.6339e+00, -3.5621e-01, -2.1239e-01,  4.6531e-01, -1.0448e-01,\n",
      "         -2.4263e-02, -6.6538e-01,  1.6839e-01,  6.0021e-01,  5.1521e-01,\n",
      "          4.5147e-01,  5.5073e-01,  5.7782e-01,  1.1097e-01,  5.2436e-05,\n",
      "          3.3933e-01,  7.6670e-02,  2.8742e-01, -3.0518e-01, -7.9520e-02,\n",
      "         -5.9840e-02, -3.3580e-01,  4.1233e-01,  1.3454e-01,  6.5992e-02,\n",
      "         -7.6054e-02,  4.7669e-01,  4.1522e-01, -5.4075e-01,  5.8509e-01,\n",
      "          7.4789e-01, -2.9202e-01,  3.0160e-01, -1.1949e+00,  3.6203e-01,\n",
      "         -7.8416e-02,  2.8469e-01, -2.0345e-01, -6.5478e-01, -1.0979e-01,\n",
      "         -4.2046e-01,  4.3797e-01, -1.2501e-01, -6.0237e-01, -1.5845e-01,\n",
      "         -5.3635e-01,  6.0682e-01, -1.6742e-01, -5.6719e-01,  2.2944e-01,\n",
      "         -1.9577e-01, -3.2329e-01, -6.3082e-01, -8.1062e-02, -5.3789e-01,\n",
      "         -4.9685e-02,  4.5224e-01, -5.1619e-01,  1.0340e-01,  3.7029e-02,\n",
      "         -4.5747e-01,  1.2105e-01, -1.2056e-01, -2.2129e-01, -5.4041e-01,\n",
      "         -4.8787e-02, -1.5656e-01, -4.3520e-01,  3.2998e-02,  2.9938e-02,\n",
      "          5.6272e-02,  2.1975e-01, -8.7189e-01, -4.7284e-01, -4.4426e-01,\n",
      "         -3.7396e-01,  1.6660e-01,  2.2330e-01, -3.5221e-01, -1.6714e-01,\n",
      "          4.7040e-01, -1.7802e-01, -2.6621e-01,  3.9659e-01,  9.8598e-02,\n",
      "          2.4214e-01,  1.0513e-01,  4.6315e-01,  3.0624e-01, -9.7298e-01,\n",
      "         -5.2594e-01, -4.3566e-01, -3.8015e-01, -1.9717e-01, -1.9236e-01,\n",
      "          1.0748e+00, -4.1077e-01,  7.1083e-01, -2.5422e-01,  3.8309e-01,\n",
      "         -2.4980e-01, -1.0813e+00,  3.1668e-01,  3.6136e-01,  8.3983e-02,\n",
      "         -2.0743e-01, -3.4126e-01, -1.5012e-01, -6.0323e-02, -6.3069e-01,\n",
      "         -6.4348e-01,  1.1128e-01,  4.5045e-01,  9.4591e-01,  7.4892e-02,\n",
      "         -4.5893e-01, -2.2867e-02,  2.9972e-01,  3.1044e-01, -3.9051e-01,\n",
      "         -4.9362e-01, -4.9510e-01,  4.8779e-01, -7.6020e-02,  1.2724e-01,\n",
      "          3.7107e-01, -4.7877e-01, -2.9640e-01,  2.0462e-01, -1.3650e-01,\n",
      "         -3.0170e-01, -8.2198e-02,  4.3180e-01, -1.0350e-01, -5.2609e-01,\n",
      "         -5.6757e-01,  2.0272e-01,  8.0487e-02,  1.4091e-01,  6.9104e-01,\n",
      "         -5.1082e-01, -8.8926e-01,  2.6057e-01, -4.2688e-01, -3.9262e-01,\n",
      "         -7.2961e-01,  4.7749e-01, -3.1587e-01,  8.0752e-01, -4.4724e-01,\n",
      "         -2.7914e-01, -1.9153e-01, -7.3017e-01, -7.7095e-01, -3.2717e-01,\n",
      "         -4.8347e-01,  3.2624e-01, -8.9121e-02, -7.8720e-01, -4.5141e-01,\n",
      "         -3.1119e-01,  1.7844e-01,  4.1582e-01, -1.3038e-01,  1.7767e-02,\n",
      "         -3.8373e-01, -1.2446e-01,  2.4670e-01,  1.3022e+00,  9.3955e-01,\n",
      "          2.1900e-01, -4.8765e-01,  3.5697e-01, -2.9744e-01,  1.9572e-01,\n",
      "         -5.5302e-01,  2.2299e-01, -6.0589e-02, -3.1414e-01,  3.3778e-01,\n",
      "          7.3365e-01, -1.5656e-01, -3.3991e-01, -5.9134e-01,  4.8607e-01,\n",
      "         -4.5492e-01,  1.2066e-02,  4.2524e-01,  7.4964e-01, -3.5876e-01,\n",
      "          7.3402e-01,  4.5885e-01,  6.4025e-01,  2.7592e-01,  2.3276e-01,\n",
      "         -2.3699e-01, -3.1940e-01,  2.0977e-01, -1.6187e-01,  3.0651e-01,\n",
      "         -1.6043e-01,  1.4219e-03, -4.5993e-01, -4.7734e-01,  4.8691e-01,\n",
      "          6.2844e-01, -3.2602e-02, -8.2304e-02,  4.7343e-01, -1.3520e-01,\n",
      "         -2.3560e-01, -1.0196e-01, -2.0630e-01,  3.5026e-01,  1.3982e-01,\n",
      "          3.9399e-01,  1.8988e-01,  8.9828e-02, -1.3286e-01,  4.5940e-01,\n",
      "          1.8423e-01,  1.1838e-01,  4.3094e-01,  4.7948e-01, -3.4881e-01,\n",
      "          9.4216e-02, -2.1832e-01,  4.9550e-01,  7.8656e-01,  4.2451e-01,\n",
      "         -5.3189e-01,  2.2975e-01, -3.8657e-03, -6.8767e-01,  2.1825e-01,\n",
      "         -1.6562e-01, -1.3950e-01, -1.1025e-01, -1.3900e-01, -3.2940e-01,\n",
      "          5.0381e-01,  4.8242e-01,  2.2298e-02, -3.7841e-01,  3.4111e-01,\n",
      "          1.4001e-01, -4.7542e-02,  1.4323e-03, -1.7394e-01, -3.6869e-01,\n",
      "          4.8293e-01, -1.1056e-01, -9.3040e-02, -2.5842e-01,  3.3285e-01,\n",
      "         -7.0239e-02,  1.1082e+00, -5.0487e-02, -5.8850e-01, -9.1165e-02,\n",
      "          4.2942e-01, -1.0500e-01,  4.8486e-01,  3.6544e-01, -1.2462e-01,\n",
      "         -7.3619e-01,  2.0991e-01, -8.1340e-01, -1.3891e-01, -4.8665e-01,\n",
      "         -2.7079e-01, -2.1866e-01,  1.9399e-01,  3.1899e-01, -1.0803e-01,\n",
      "          8.2527e-02,  8.4414e-01, -2.8611e-01, -7.5957e-01,  7.1129e-01,\n",
      "          2.1381e-01, -4.9554e-01, -5.1547e-01, -2.2507e-01, -8.8687e-01,\n",
      "         -1.6529e-01,  2.2780e-01,  1.0499e-01, -3.2690e-01,  2.8916e-01,\n",
      "         -4.8616e-01, -1.5593e-01,  1.7598e-01, -2.3226e-01, -8.5751e-02,\n",
      "          9.6454e-03,  5.7576e-01,  5.8593e-01,  4.9452e-01,  2.5615e-01,\n",
      "         -2.0744e-01,  3.0028e-01, -5.7111e-01, -7.7715e-01, -1.6411e-01,\n",
      "         -3.0774e-01,  6.7518e-01, -6.9691e-02, -3.2750e-01, -1.6398e-01,\n",
      "         -2.7161e-01, -4.1715e-01, -5.6067e-01, -5.9116e-03,  4.9204e-01,\n",
      "          3.6901e-01,  3.9663e-01,  3.2516e-01,  1.7016e-01, -5.3043e-01,\n",
      "          7.6393e-01,  4.6181e-01,  2.5185e-01, -2.8930e-01, -5.0259e-01,\n",
      "         -1.5783e-01,  2.3773e-02,  1.0187e-01, -1.5007e-01, -8.3932e-01,\n",
      "          1.0647e-01, -3.6842e-01, -2.8063e-01,  2.6200e-01, -8.3418e-01,\n",
      "          2.3678e-01,  3.9125e-01,  9.2006e-03, -2.9849e-01,  4.8016e-01,\n",
      "         -2.5229e-01, -2.5832e-01, -7.7461e-01,  2.4423e-01, -4.8160e-01,\n",
      "         -4.5917e-01, -6.4882e-01, -5.0628e-02, -1.4891e-01, -1.9742e-01,\n",
      "         -8.0739e-01,  5.7040e-01, -6.6410e-02,  6.3668e-01,  1.7940e-01,\n",
      "         -3.0420e-01, -4.5101e-01, -2.9974e-01, -5.0577e-01,  3.6798e-01,\n",
      "          1.9032e-01,  1.9031e-01,  1.1117e-02, -9.4096e-02,  2.7357e-01,\n",
      "         -8.0626e-01,  1.0404e-01, -2.9843e-01,  6.2416e-01, -1.7304e-01,\n",
      "          1.1009e-01,  4.2159e-01, -5.5203e-02,  4.5356e-01, -1.8230e-01,\n",
      "         -5.0028e-01,  1.1320e-01, -5.6668e-01,  1.6422e-01, -7.7336e-01,\n",
      "         -8.0147e-01, -1.6244e-02, -2.3491e-02, -9.7563e-02, -6.0048e-01,\n",
      "         -6.1589e-02, -4.5987e-01, -5.2482e-01, -3.2665e-01,  8.9343e-01,\n",
      "          6.6122e-01, -3.9811e-02,  5.9644e-02,  4.8312e-01, -8.9176e-01,\n",
      "         -2.9466e-01,  5.7079e-03,  2.7393e-01,  3.7044e-01, -3.5416e-01,\n",
      "         -5.5662e-01,  5.3228e-01,  5.1381e-01,  9.1887e-01,  3.5974e-01,\n",
      "         -6.0870e-01,  4.5881e-01,  2.7970e-02,  2.2801e-01,  1.8588e-01,\n",
      "          4.6409e-01, -3.1644e-01, -2.5253e-01,  6.7383e-01, -4.9919e-01,\n",
      "          3.4086e-01, -4.9836e-01, -4.1597e-01,  3.9263e-01,  1.3415e-01,\n",
      "          5.1100e-01, -3.7225e-01, -8.5528e-01]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JH\\miniconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from bert import BERT\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def convert_to_tensor(corpus, tokenizer, device):\n",
    "    inputs = tokenizer(corpus,\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\",\n",
    "                       max_length=50,\n",
    "                       pad_to_max_length=\"right\")\n",
    "    \n",
    "    embedding = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "        \n",
    "    inputs = {'source': torch.LongTensor(embedding).to(device),\n",
    "              'token_type_ids': torch.LongTensor(token_type_ids).to(device),\n",
    "              'attention_mask': attention_mask.to(device)}\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# def pytorch_cos_sim(a, b):\n",
    "#     return torch.nn.functional.cosine_similarity(a, b)\n",
    "\n",
    "def pytorch_cos_sim(a, b):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    This function can be used as a faster replacement for 1-scipy.spatial.distance.cdist(a,b)\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = a / a.norm(dim=1)[:, None]\n",
    "    b_norm = b / b.norm(dim=1)[:, None]\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BERT(AutoModel.from_pretrained('BM-K/KoSimCSE-roberta'))\n",
    "tokenizer = AutoTokenizer.from_pretrained('BM-K/KoSimCSE-roberta')\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['한 남자가 음식을 먹는다.',\n",
    "            '한 남자가 빵 한 조각을 먹는다.',\n",
    "            '그 여자가 아이를 돌본다.',\n",
    "            '한 남자가 말을 탄다.',\n",
    "            '한 여자가 바이올린을 연주한다.',\n",
    "            '두 남자가 수레를 숲 속으로 밀었다.',\n",
    "            '한 남자가 담으로 싸인 땅에서 백마를 타고 있다.',\n",
    "            '원숭이 한 마리가 드럼을 연주한다.',\n",
    "            '치타 한 마리가 먹이 뒤에서 달리고 있다.']\n",
    "\n",
    "inputs_corpus = convert_to_tensor(corpus, tokenizer, device)\n",
    "\n",
    "corpus_embeddings = model.encode(inputs_corpus, device)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['1',\n",
    "            '고릴라 의상을 입은 누군가가 드럼을 연주하고 있다.',\n",
    "            '치타가 들판을 가로 질러 먹이를 쫓는다.']\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = 5\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(convert_to_tensor([query], tokenizer, device), device)\n",
    "\n",
    "    print(query_embedding)\n",
    "    break\n",
    "\n",
    "    # cos_scores = pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    # cos_scores = cos_scores.cpu().detach().numpy()\n",
    "\n",
    "    # top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "    # print(\"\\n\\n======================\\n\\n\")\n",
    "    # print(\"Query:\", query)\n",
    "    # print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    # for idx in top_results[0:top_k]:\n",
    "    #     print(corpus[idx].strip(), \"(Score: %.4f)\" % (cos_scores[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "kw_model = KeyBERT(model=sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JH\\miniconda3\\envs\\capstone\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]c:\\Users\\JH\\miniconda3\\envs\\capstone\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\JH\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 23 files: 100%|██████████| 23/23 [00:48<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.626  0.3474]\n",
      " [0.3499 0.678 ]]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n",
    "\n",
    "embeddings_1 = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)\n",
    "# [[0.6265, 0.3477], [0.3499, 0.678 ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_vecs': array([[-0.0365  ,  0.02264 , -0.0603  , ..., -0.04626 , -0.04446 ,\n",
       "         -0.0569  ],\n",
       "        [-0.0192  ,  0.03555 , -0.04645 , ..., -0.006733, -0.011765,\n",
       "         -0.03653 ],\n",
       "        [-0.00999 ,  0.0509  , -0.03732 , ..., -0.02893 , -0.03296 ,\n",
       "         -0.007736],\n",
       "        [ 0.041   ,  0.02959 , -0.0346  , ...,  0.010056, -0.04718 ,\n",
       "         -0.0789  ],\n",
       "        [ 0.00789 ,  0.0373  , -0.0435  , ..., -0.0377  , -0.0432  ,\n",
       "         -0.01268 ]], dtype=float16),\n",
       " 'lexical_weights': None,\n",
       " 'colbert_vecs': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode(list(X_train[5:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.374  0.374  0.374  0.374  0.4043]\n",
      " [0.4565 0.4565 0.4565 0.4565 0.5005]\n",
      " [0.3787 0.3787 0.3787 0.3787 0.454 ]\n",
      " [0.4043 0.4043 0.4043 0.4043 0.39  ]\n",
      " [0.3813 0.3813 0.3813 0.3813 0.4026]]\n"
     ]
    }
   ],
   "source": [
    "x_train_encode = model.encode(list(X_train[5:10]))['dense_vecs']\n",
    "y_train_encode = model.encode(list(y_train[5:10]))['dense_vecs']\n",
    "\n",
    "similarity = x_train_encode @ y_train_encode.T\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
