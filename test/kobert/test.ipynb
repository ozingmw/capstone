{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jh/miniconda3/envs/capstone/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Tokenization classes for KoBERT model \"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\n",
    "    \"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
    "    \"vocab_txt\": \"vocab.txt\",\n",
    "}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\",\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"monologg/kobert\": 512,\n",
    "    \"monologg/kobert-lm\": 512,\n",
    "    \"monologg/distilkobert\": 512,\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
    "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
    "    \"monologg/distilkobert\": {\"do_lower_case\": False},\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = \"‚ñÅ\"\n",
    "\n",
    "\n",
    "class KoBertTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    SentencePiece based tokenizer. Peculiarities:\n",
    "        - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        vocab_txt,\n",
    "        do_lower_case=False,\n",
    "        remove_space=True,\n",
    "        keep_accents=False,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        # Build vocab\n",
    "        self.token2idx = dict()\n",
    "        self.idx2token = []\n",
    "        with open(vocab_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\n",
    "                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                \"pip install sentencepiece\"\n",
    "            )\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.remove_space = remove_space\n",
    "        self.keep_accents = keep_accents\n",
    "        self.vocab_file = vocab_file\n",
    "        self.vocab_txt = vocab_txt\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\n",
    "                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                \"pip install sentencepiece\"\n",
    "            )\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def preprocess_text(self, inputs):\n",
    "        if self.remove_space:\n",
    "            outputs = \" \".join(inputs.strip().split())\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "        if not self.keep_accents:\n",
    "            outputs = unicodedata.normalize(\"NFKD\", outputs)\n",
    "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
    "        if self.do_lower_case:\n",
    "            outputs = outputs.lower()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Tokenize a string.\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        pieces = self.sp_model.encode(text, out_type=str)\n",
    "        new_pieces = []\n",
    "        for piece in pieces:\n",
    "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
    "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
    "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "                    if len(cur_pieces[0]) == 1:\n",
    "                        cur_pieces = cur_pieces[1:]\n",
    "                    else:\n",
    "                        cur_pieces[0] = cur_pieces[0][1:]\n",
    "                cur_pieces.append(piece[-1])\n",
    "                new_pieces.extend(cur_pieces)\n",
    "            else:\n",
    "                new_pieces.append(piece)\n",
    "\n",
    "        return new_pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
    "        return self.idx2token[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
    "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A KoBERT sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(\n",
    "                map(\n",
    "                    lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0,\n",
    "                    token_ids_0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A KoBERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\"Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "        to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        # 1. Save sentencepiece model\n",
    "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
    "            copyfile(self.vocab_file, out_vocab_model)\n",
    "\n",
    "        # 2. Save vocab.txt\n",
    "        index = 0\n",
    "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
    "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return out_vocab_model, out_vocab_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at monologg/distilkobert and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('monologg/distilkobert', num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Ïó∞Î†π</th>\n",
       "      <th>ÏÑ±Î≥Ñ</th>\n",
       "      <th>ÏÉÅÌô©ÌÇ§ÏõåÎìú</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>Ï§ëÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ</td>\n",
       "      <td>4</td>\n",
       "      <td>ÏßÄÍ∏àÍπåÏßÄ ÌûòÎì§Í≤å ÏùºÌñàÎäîÎç∞ ÏùÄÌá¥Ìï¥ÏÑú ÎèàÏù¥ ÏóÜÎã§Í≥† ÌïòÎãà ÏûêÏãùÏù¥ ÌôîÎ•º ÎÇ¥ÏÑú ÏÉÅÏ≤òÎ•º Î∞õÏïòÏñ¥.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>Ï§ëÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ</td>\n",
       "      <td>4</td>\n",
       "      <td>ÏπúÍµ¨ÌïúÌÖå ÏùÄÌá¥Ìï† Í±∞ÎùºÍ≥† ÏñòÍ∏∞ÌñàÎçîÎãà ÏïûÏúºÎ°ú Î≠ò Î®πÍ≥† ÏÇ¥ Í±∞ÎÉêÎ©¥ÏÑú ÎπÑÏõÉÎçîÎùºÍ≥†. Í∏∞Î∂ÑÏù¥ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>307</td>\n",
       "      <td>Ï§ëÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ</td>\n",
       "      <td>4</td>\n",
       "      <td>ÏπúÍµ¨ÌïúÌÖå ÏùÄÌá¥ÌïúÎã§Í≥† ÌñàÎçîÎãà Í∑∏Í≤å ÎßêÏù¥ÎÇò ÎêòÎäî Í±∞ÎÉêÎ©∞ ÎÇ† ÌïúÏã¨Ìïú ÏÇ¨Îûå Ï∑®Í∏âÌï¥ÏÑú ÏÑúÏö¥ÌñàÏñ¥.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>308</td>\n",
       "      <td>Ï§ëÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ</td>\n",
       "      <td>4</td>\n",
       "      <td>Í∑∏ÎèôÏïà Ïó¥Ïã¨Ìûà Îã¨Î†§ÏôÄÏÑú Ï¢Ä Ïâ¨Î†§Í≥† ÌïòÎäîÎç∞ ÏùÄÌá¥ÌïúÎã§Í≥† ÌïòÎãà Ï£ºÎ≥ÄÏóêÏÑú Îã§ ÎßêÎ†§ÏÑú Í∏∞Î∂ÑÏù¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>Ï§ëÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ</td>\n",
       "      <td>4</td>\n",
       "      <td>ÎßéÏùÄ Í≥†ÎØº ÌõÑ ÏùÄÌá¥Î•º Í≤∞Ïã¨ÌñàÎäîÎç∞ Ï£ºÎ≥ÄÏóêÏÑú Îã§Îì§ ÏÑ£Î∂ÄÎ•∏ ÏÉùÍ∞ÅÏù¥ÎùºÍ≥† Ìï¥ÏÑú ÎßàÏùåÏù¥ Ïïà Ï¢ãÏïÑ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51625</th>\n",
       "      <td>51626</td>\n",
       "      <td>ÎÖ∏ÎÖÑ</td>\n",
       "      <td>ÎÇ®ÏÑ±</td>\n",
       "      <td>Ïû¨Ï†ï</td>\n",
       "      <td>2</td>\n",
       "      <td>ÎÇòÏù¥Í∞Ä Î®πÍ≥† Ïù¥Ï†ú ÎèàÎèÑ Î™ª Î≤åÏñ¥ Ïò§ÎãàÍπå Ïñ¥ÎñªÍ≤å ÏÇ¥ÏïÑÍ∞ÄÏïº Ìï†ÏßÄ ÎßâÎßâÌï¥. Îä•Î†•ÎèÑ ÏóÜÍ≥†.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51626</th>\n",
       "      <td>51627</td>\n",
       "      <td>ÎÖ∏ÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>Ïû¨Ï†ï</td>\n",
       "      <td>3</td>\n",
       "      <td>Î™∏Ïù¥ ÎßéÏù¥ ÏïΩÌï¥Ï°åÎÇò Î¥ê. Ïù¥Ï†ú Ï†ÑÍ≥º Í∞ôÏù¥ ÏùºÌïòÏßÄ Î™ªÌï† Í≤É Í∞ôÏïÑ ÎÑàÎ¨¥ ÏßúÏ¶ù ÎÇò.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51627</th>\n",
       "      <td>51628</td>\n",
       "      <td>ÎÖ∏ÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>Ïû¨Ï†ï</td>\n",
       "      <td>4</td>\n",
       "      <td>Ïù¥Ï†ú Ïñ¥ÎñªÍ≤å Ìï¥Ïïº Ìï†ÏßÄ Î™®Î•¥Í≤†Ïñ¥. ÎÇ®Ìé∏ÎèÑ Í∑∏Î†áÍ≥† ÎÖ∏ÌõÑ Ï§ÄÎπÑÎèÑ Ïïà ÎêòÏñ¥ÏÑú ÎØ∏ÎûòÍ∞Ä Í±±Ï†ïÎèº.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51628</th>\n",
       "      <td>51629</td>\n",
       "      <td>ÎÖ∏ÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>ÎåÄÏù∏Í¥ÄÍ≥Ñ</td>\n",
       "      <td>3</td>\n",
       "      <td>Î™áÏã≠ ÎÖÑÏùÑ Ìï®Íªò ÏÇ¥ÏïòÎçò ÎÇ®Ìé∏Í≥º Ïù¥ÌòºÌñàÏñ¥. Í∑∏ÎèôÏïàÏùò ÏÑ∏ÏõîÏóê Î∞∞Ïã†Í∞êÏùÑ ÎäêÎÅºÍ≥† ÎÑàÎ¨¥ ÌôîÍ∞Ä ÎÇò.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51629</th>\n",
       "      <td>51630</td>\n",
       "      <td>ÎÖ∏ÎÖÑ</td>\n",
       "      <td>Ïó¨ÏÑ±</td>\n",
       "      <td>ÎåÄÏù∏Í¥ÄÍ≥Ñ</td>\n",
       "      <td>4</td>\n",
       "      <td>ÎÇ®Ìé∏Í≥º Í≤∞ÌòºÌïú ÏßÄ ÏÇ¨Ïã≠ ÎÖÑÏù¥Ïïº. Ïù¥Ï†ú ÏÇ¨Îûå ÎßåÎÇòÎäî Í≤ÉÎèÑ Î≤ÑÍ≤ÅÍ≥† ÏïåÎçò ÏÇ¨ÎûåÎèÑ Ï†êÏ†ê ÏÇ¨ÎùºÏ†∏.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42694 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Ïó∞Î†π  ÏÑ±Î≥Ñ       ÏÉÅÌô©ÌÇ§ÏõåÎìú  label  \\\n",
       "304           305  Ï§ëÎÖÑ  Ïó¨ÏÑ±  Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ      4   \n",
       "305           306  Ï§ëÎÖÑ  Ïó¨ÏÑ±  Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ      4   \n",
       "306           307  Ï§ëÎÖÑ  Ïó¨ÏÑ±  Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ      4   \n",
       "307           308  Ï§ëÎÖÑ  Ïó¨ÏÑ±  Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ      4   \n",
       "308           309  Ï§ëÎÖÑ  Ïó¨ÏÑ±  Ïû¨Ï†ï,ÏùÄÌá¥,ÎÖ∏ÌõÑÏ§ÄÎπÑ      4   \n",
       "...           ...  ..  ..         ...    ...   \n",
       "51625       51626  ÎÖ∏ÎÖÑ  ÎÇ®ÏÑ±          Ïû¨Ï†ï      2   \n",
       "51626       51627  ÎÖ∏ÎÖÑ  Ïó¨ÏÑ±          Ïû¨Ï†ï      3   \n",
       "51627       51628  ÎÖ∏ÎÖÑ  Ïó¨ÏÑ±          Ïû¨Ï†ï      4   \n",
       "51628       51629  ÎÖ∏ÎÖÑ  Ïó¨ÏÑ±        ÎåÄÏù∏Í¥ÄÍ≥Ñ      3   \n",
       "51629       51630  ÎÖ∏ÎÖÑ  Ïó¨ÏÑ±        ÎåÄÏù∏Í¥ÄÍ≥Ñ      4   \n",
       "\n",
       "                                                    text  \n",
       "304      ÏßÄÍ∏àÍπåÏßÄ ÌûòÎì§Í≤å ÏùºÌñàÎäîÎç∞ ÏùÄÌá¥Ìï¥ÏÑú ÎèàÏù¥ ÏóÜÎã§Í≥† ÌïòÎãà ÏûêÏãùÏù¥ ÌôîÎ•º ÎÇ¥ÏÑú ÏÉÅÏ≤òÎ•º Î∞õÏïòÏñ¥.  \n",
       "305    ÏπúÍµ¨ÌïúÌÖå ÏùÄÌá¥Ìï† Í±∞ÎùºÍ≥† ÏñòÍ∏∞ÌñàÎçîÎãà ÏïûÏúºÎ°ú Î≠ò Î®πÍ≥† ÏÇ¥ Í±∞ÎÉêÎ©¥ÏÑú ÎπÑÏõÉÎçîÎùºÍ≥†. Í∏∞Î∂ÑÏù¥ ...  \n",
       "306     ÏπúÍµ¨ÌïúÌÖå ÏùÄÌá¥ÌïúÎã§Í≥† ÌñàÎçîÎãà Í∑∏Í≤å ÎßêÏù¥ÎÇò ÎêòÎäî Í±∞ÎÉêÎ©∞ ÎÇ† ÌïúÏã¨Ìïú ÏÇ¨Îûå Ï∑®Í∏âÌï¥ÏÑú ÏÑúÏö¥ÌñàÏñ¥.  \n",
       "307    Í∑∏ÎèôÏïà Ïó¥Ïã¨Ìûà Îã¨Î†§ÏôÄÏÑú Ï¢Ä Ïâ¨Î†§Í≥† ÌïòÎäîÎç∞ ÏùÄÌá¥ÌïúÎã§Í≥† ÌïòÎãà Ï£ºÎ≥ÄÏóêÏÑú Îã§ ÎßêÎ†§ÏÑú Í∏∞Î∂ÑÏù¥...  \n",
       "308     ÎßéÏùÄ Í≥†ÎØº ÌõÑ ÏùÄÌá¥Î•º Í≤∞Ïã¨ÌñàÎäîÎç∞ Ï£ºÎ≥ÄÏóêÏÑú Îã§Îì§ ÏÑ£Î∂ÄÎ•∏ ÏÉùÍ∞ÅÏù¥ÎùºÍ≥† Ìï¥ÏÑú ÎßàÏùåÏù¥ Ïïà Ï¢ãÏïÑ.  \n",
       "...                                                  ...  \n",
       "51625     ÎÇòÏù¥Í∞Ä Î®πÍ≥† Ïù¥Ï†ú ÎèàÎèÑ Î™ª Î≤åÏñ¥ Ïò§ÎãàÍπå Ïñ¥ÎñªÍ≤å ÏÇ¥ÏïÑÍ∞ÄÏïº Ìï†ÏßÄ ÎßâÎßâÌï¥. Îä•Î†•ÎèÑ ÏóÜÍ≥†.  \n",
       "51626        Î™∏Ïù¥ ÎßéÏù¥ ÏïΩÌï¥Ï°åÎÇò Î¥ê. Ïù¥Ï†ú Ï†ÑÍ≥º Í∞ôÏù¥ ÏùºÌïòÏßÄ Î™ªÌï† Í≤É Í∞ôÏïÑ ÎÑàÎ¨¥ ÏßúÏ¶ù ÎÇò.  \n",
       "51627   Ïù¥Ï†ú Ïñ¥ÎñªÍ≤å Ìï¥Ïïº Ìï†ÏßÄ Î™®Î•¥Í≤†Ïñ¥. ÎÇ®Ìé∏ÎèÑ Í∑∏Î†áÍ≥† ÎÖ∏ÌõÑ Ï§ÄÎπÑÎèÑ Ïïà ÎêòÏñ¥ÏÑú ÎØ∏ÎûòÍ∞Ä Í±±Ï†ïÎèº.  \n",
       "51628  Î™áÏã≠ ÎÖÑÏùÑ Ìï®Íªò ÏÇ¥ÏïòÎçò ÎÇ®Ìé∏Í≥º Ïù¥ÌòºÌñàÏñ¥. Í∑∏ÎèôÏïàÏùò ÏÑ∏ÏõîÏóê Î∞∞Ïã†Í∞êÏùÑ ÎäêÎÅºÍ≥† ÎÑàÎ¨¥ ÌôîÍ∞Ä ÎÇò.  \n",
       "51629  ÎÇ®Ìé∏Í≥º Í≤∞ÌòºÌïú ÏßÄ ÏÇ¨Ïã≠ ÎÖÑÏù¥Ïïº. Ïù¥Ï†ú ÏÇ¨Îûå ÎßåÎÇòÎäî Í≤ÉÎèÑ Î≤ÑÍ≤ÅÍ≥† ÏïåÎçò ÏÇ¨ÎûåÎèÑ Ï†êÏ†ê ÏÇ¨ÎùºÏ†∏.  \n",
       "\n",
       "[42694 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "train_data = pd.read_csv(\"../../datasets/train.csv\", encoding = 'utf-8')\n",
    "\n",
    "train_data.dropna(inplace = True)\n",
    "train_data.drop(['Í∞êÏ†ï_ÏÜåÎ∂ÑÎ•ò','ÏãúÏä§ÌÖúÎ¨∏Ïû•1','ÏãúÏä§ÌÖúÎ¨∏Ïû•2','ÏãúÏä§ÌÖúÎ¨∏Ïû•3', 'ÏÇ¨ÎûåÎ¨∏Ïû•2', 'ÏÇ¨ÎûåÎ¨∏Ïû•3', 'Ïã†Ï≤¥ÏßàÌôò'], axis=1, inplace=True)\n",
    "train_data.rename({'ÏÇ¨ÎûåÎ¨∏Ïû•1':'text', 'Í∞êÏ†ï_ÎåÄÎ∂ÑÎ•ò':'label'}, axis=1, inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['label'] = label_encoder.fit_transform(train_data['label'])\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42694/42694 [00:04<00:00, 10428.00 examples/s]\n",
      "Stringifying the column: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42694/42694 [00:00<00:00, 604618.30 examples/s]\n",
      "Casting to class labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42694/42694 [00:00<00:00, 528209.92 examples/s]\n",
      "/Users/jh/miniconda3/envs/capstone/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "datasets = Dataset.from_dict(train_data[['text', 'label']].to_dict('list'))\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.class_encode_column(\"label\")\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1, stratify_by_column='label', shuffle=True)\n",
    "\n",
    "train_dataset = tokenized_datasets['train'].shuffle(seed=42)\n",
    "val_dataset = tokenized_datasets['test'].shuffle(seed=42)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoint\",\n",
    "    logging_dir=\"logs\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
